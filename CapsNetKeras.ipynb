{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CapsNetKeras.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sQLJjViPzNi",
        "colab_type": "text"
      },
      "source": [
        "Keras implementation of CapsNet in Hinton's paper Dynamic Routing Between Capsules.\n",
        "The current version maybe only works for TensorFlow backend. Actually it will be straightforward to re-write to TF code.\n",
        "Adopting to other backends should be easy, but I have not tested this. \n",
        "\n",
        "Usage:\n",
        "       python capsulenet.py\n",
        "       python capsulenet.py --epochs 50\n",
        "       python capsulenet.py --epochs 50 --routings 3\n",
        "       ... ...\n",
        "       \n",
        "Result:\n",
        "    Validation accuracy > 99.5% after 20 epochs. Converge to 99.66% after 50 epochs.\n",
        "    About 110 seconds per epoch on a single GTX1070 GPU card\n",
        "    \n",
        "Author: Xifeng Guo, E-mail: `guoxifeng1990@163.com`, Github: `https://github.com/XifengGuo/CapsNet-Keras`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7nowM_vjMwx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b4a490e2-45ce-4e0b-8034-7af35a2f7289"
      },
      "source": [
        "\"\"\"\n",
        "Keras implementation of CapsNet in Hinton's paper Dynamic Routing Between Capsules.\n",
        "The current version maybe only works for TensorFlow backend. Actually it will be straightforward to re-write to TF code.\n",
        "Adopting to other backends should be easy, but I have not tested this.\n",
        "\n",
        "Usage:\n",
        "       python capsulenet.py\n",
        "       python capsulenet.py --epochs 50\n",
        "       python capsulenet.py --epochs 50 --routings 3\n",
        "       ... ...\n",
        "\n",
        "Result:\n",
        "    Validation accuracy > 99.5% after 20 epochs. Converge to 99.66% after 50 epochs.\n",
        "    About 110 seconds per epoch on a single GTX1070 GPU card\n",
        "\n",
        "Author: Xifeng Guo, E-mail: `guoxifeng1990@163.com`, Github: `https://github.com/XifengGuo/CapsNet-Keras`\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from keras import layers, models, optimizers\n",
        "from keras import backend as K\n",
        "from keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "from utils import combine_images\n",
        "from PIL import Image\n",
        "from capsulelayers import CapsuleLayer, PrimaryCap, Length, Mask\n",
        "\n",
        "K.set_image_data_format('channels_last')\n",
        "\n",
        "\n",
        "def CapsNet(input_shape, n_class, routings):\n",
        "    \"\"\"\n",
        "    A Capsule Network on MNIST.\n",
        "    :param input_shape: data shape, 3d, [width, height, channels]\n",
        "    :param n_class: number of classes\n",
        "    :param routings: number of routing iterations\n",
        "    :return: Two Keras Models, the first one used for training, and the second one for evaluation.\n",
        "            `eval_model` can also be used for training.\n",
        "    \"\"\"\n",
        "    x = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Layer 1: Just a conventional Conv2D layer\n",
        "    conv1 = layers.Conv2D(filters=256, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv1')(x)\n",
        "\n",
        "    # Layer 2: Conv2D layer with `squash` activation, then reshape to [None, num_capsule, dim_capsule]\n",
        "    primarycaps = PrimaryCap(conv1, dim_capsule=8, n_channels=32, kernel_size=9, strides=2, padding='valid')\n",
        "\n",
        "    # Layer 3: Capsule layer. Routing algorithm works here.\n",
        "    digitcaps = CapsuleLayer(num_capsule=n_class, dim_capsule=16, routings=routings,\n",
        "                             name='digitcaps')(primarycaps)\n",
        "\n",
        "    # Layer 4: This is an auxiliary layer to replace each capsule with its length. Just to match the true label's shape.\n",
        "    # If using tensorflow, this will not be necessary. :)\n",
        "    out_caps = Length(name='capsnet')(digitcaps)\n",
        "\n",
        "    # Decoder network.\n",
        "    y = layers.Input(shape=(n_class,))\n",
        "    masked_by_y = Mask()([digitcaps, y])  # The true label is used to mask the output of capsule layer. For training\n",
        "    masked = Mask()(digitcaps)  # Mask using the capsule with maximal length. For prediction\n",
        "\n",
        "    # Shared Decoder model in training and prediction\n",
        "    decoder = models.Sequential(name='decoder')\n",
        "    decoder.add(layers.Dense(512, activation='relu', input_dim=16*n_class))\n",
        "    decoder.add(layers.Dense(1024, activation='relu'))\n",
        "    decoder.add(layers.Dense(np.prod(input_shape), activation='sigmoid'))\n",
        "    decoder.add(layers.Reshape(target_shape=input_shape, name='out_recon'))\n",
        "\n",
        "    # Models for training and evaluation (prediction)\n",
        "    train_model = models.Model([x, y], [out_caps, decoder(masked_by_y)])\n",
        "    eval_model = models.Model(x, [out_caps, decoder(masked)])\n",
        "\n",
        "    # manipulate model\n",
        "    noise = layers.Input(shape=(n_class, 16))\n",
        "    noised_digitcaps = layers.Add()([digitcaps, noise])\n",
        "    masked_noised_y = Mask()([noised_digitcaps, y])\n",
        "    manipulate_model = models.Model([x, y, noise], decoder(masked_noised_y))\n",
        "    return train_model, eval_model, manipulate_model\n",
        "\n",
        "\n",
        "def margin_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Margin loss for Eq.(4). When y_true[i, :] contains not just one `1`, this loss should work too. Not test it.\n",
        "    :param y_true: [None, n_classes]\n",
        "    :param y_pred: [None, num_capsule]\n",
        "    :return: a scalar loss value.\n",
        "    \"\"\"\n",
        "    L = y_true * K.square(K.maximum(0., 0.9 - y_pred)) + \\\n",
        "        0.5 * (1 - y_true) * K.square(K.maximum(0., y_pred - 0.1))\n",
        "\n",
        "    return K.mean(K.sum(L, 1))\n",
        "\n",
        "\n",
        "def train(model, data, args):\n",
        "    \"\"\"\n",
        "    Training a CapsuleNet\n",
        "    :param model: the CapsuleNet model\n",
        "    :param data: a tuple containing training and testing data, like `((x_train, y_train), (x_test, y_test))`\n",
        "    :param args: arguments\n",
        "    :return: The trained model\n",
        "    \"\"\"\n",
        "    # unpacking the data\n",
        "    (x_train, y_train), (x_test, y_test) = data\n",
        "\n",
        "    # callbacks\n",
        "    log = callbacks.CSVLogger(args.save_dir + '/log.csv')\n",
        "    #tb = callbacks.TensorBoard(log_dir=args.save_dir + '/tensorboard-logs',\n",
        "                               #batch_size=args.batch_size, histogram_freq=int(args.debug))\n",
        "    checkpoint = callbacks.ModelCheckpoint(args.save_dir + '/weights-{epoch:02d}.h5', monitor='val_capsnet_acc',\n",
        "                                           save_best_only=True, save_weights_only=True, verbose=1)\n",
        "    lr_decay = callbacks.LearningRateScheduler(schedule=lambda epoch: args.lr * (args.lr_decay ** epoch))\n",
        "\n",
        "    # compile the model\n",
        "    model.compile(optimizer=optimizers.Adam(lr=args.lr),\n",
        "                  loss=[margin_loss, 'mse'],\n",
        "                  loss_weights=[1., args.lam_recon],\n",
        "                  metrics={'capsnet': 'accuracy'})\n",
        "\n",
        "    \"\"\"\n",
        "    # Training without data augmentation:\n",
        "    model.fit([x_train, y_train], [y_train, x_train], batch_size=args.batch_size, epochs=args.epochs,\n",
        "              validation_data=[[x_test, y_test], [y_test, x_test]], callbacks=[log, tb, checkpoint, lr_decay])\n",
        "    \"\"\"\n",
        "\n",
        "    # Begin: Training with data augmentation ---------------------------------------------------------------------#\n",
        "    def train_generator(x, y, batch_size, shift_fraction=0.):\n",
        "        train_datagen = ImageDataGenerator(width_shift_range=shift_fraction,\n",
        "                                           height_shift_range=shift_fraction)  # shift up to 2 pixel for MNIST\n",
        "        generator = train_datagen.flow(x, y, batch_size=batch_size)\n",
        "        while 1:\n",
        "            x_batch, y_batch = generator.next()\n",
        "            yield ([x_batch, y_batch], [y_batch, x_batch])\n",
        "\n",
        "    # Training with data augmentation. If shift_fraction=0., also no augmentation.\n",
        "    model.fit_generator(generator=train_generator(x_train, y_train, args.batch_size, args.shift_fraction),\n",
        "                        steps_per_epoch=int(y_train.shape[0] / args.batch_size),\n",
        "                        epochs=args.epochs,\n",
        "                        validation_data=[[x_test, y_test], [y_test, x_test]],\n",
        "                        callbacks=[log, checkpoint, lr_decay])\n",
        "    # End: Training with data augmentation -----------------------------------------------------------------------#\n",
        "\n",
        "    model.save_weights(args.save_dir + '/trained_model.h5')\n",
        "    print('Trained model saved to \\'%s/trained_model.h5\\'' % args.save_dir)\n",
        "\n",
        "    from utils import plot_log\n",
        "    plot_log(args.save_dir + '/log.csv', show=True)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def test(model, data, args):\n",
        "    x_test, y_test = data\n",
        "    y_pred, x_recon = model.predict(x_test, batch_size=100)\n",
        "    print('-'*30 + 'Begin: test' + '-'*30)\n",
        "    print('Test acc:', np.sum(np.argmax(y_pred, 1) == np.argmax(y_test, 1))/y_test.shape[0])\n",
        "\n",
        "    img = combine_images(np.concatenate([x_test[:50],x_recon[:50]]))\n",
        "    image = img * 255\n",
        "    Image.fromarray(image.astype(np.uint8)).save(args.save_dir + \"/real_and_recon.png\")\n",
        "    print()\n",
        "    print('Reconstructed images are saved to %s/real_and_recon.png' % args.save_dir)\n",
        "    print('-' * 30 + 'End: test' + '-' * 30)\n",
        "    plt.imshow(plt.imread(args.save_dir + \"/real_and_recon.png\"))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def manipulate_latent(model, data, args):\n",
        "    print('-'*30 + 'Begin: manipulate' + '-'*30)\n",
        "    x_test, y_test = data\n",
        "    index = np.argmax(y_test, 1) == args.digit\n",
        "    number = np.random.randint(low=0, high=sum(index) - 1)\n",
        "    x, y = x_test[index][number], y_test[index][number]\n",
        "    x, y = np.expand_dims(x, 0), np.expand_dims(y, 0)\n",
        "    noise = np.zeros([1, 10, 16])\n",
        "    x_recons = []\n",
        "    for dim in range(16):\n",
        "        for r in [-0.25, -0.2, -0.15, -0.1, -0.05, 0, 0.05, 0.1, 0.15, 0.2, 0.25]:\n",
        "            tmp = np.copy(noise)\n",
        "            tmp[:,:,dim] = r\n",
        "            x_recon = model.predict([x, y, tmp])\n",
        "            x_recons.append(x_recon)\n",
        "\n",
        "    x_recons = np.concatenate(x_recons)\n",
        "\n",
        "    img = combine_images(x_recons, height=16)\n",
        "    image = img*255\n",
        "    Image.fromarray(image.astype(np.uint8)).save(args.save_dir + '/manipulate-%d.png' % args.digit)\n",
        "    print('manipulated result saved to %s/manipulate-%d.png' % (args.save_dir, args.digit))\n",
        "    print('-' * 30 + 'End: manipulate' + '-' * 30)\n",
        "\n",
        "\n",
        "def load_mnist():\n",
        "    # the data, shuffled and split between train and test sets\n",
        "    from keras.datasets import mnist\n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "    x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.\n",
        "    x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.\n",
        "    y_train = to_categorical(y_train.astype('float32'))\n",
        "    y_test = to_categorical(y_test.astype('float32'))\n",
        "    return (x_train, y_train), (x_test, y_test)\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import callbacks\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # load data\n",
        "(x_train, y_train), (x_test, y_test) = load_mnist()\n",
        "\n",
        "    # define model\n",
        "model, eval_model, manipulate_model = CapsNet(input_shape=x_train.shape[1:],\n",
        "                                                  n_class=len(np.unique(np.argmax(y_train, 1))),\n",
        "                                                  routings=3)\n",
        "model.summary()\n",
        "\n",
        "#train(model=model, data=((x_train, y_train), (x_test, y_test)), args=args)\n",
        "\n",
        "data=((x_train, y_train), (x_test, y_test))\n",
        "\n",
        "model.compile(optimizer=optimizers.Adam(lr=0.001),\n",
        "                  loss=[margin_loss, 'mse'],\n",
        "                  loss_weights=[1., 0.392],\n",
        "                  metrics={'capsnet': 'accuracy'})\n",
        "\n",
        "model.fit([x_train, y_train], [y_train, x_train], batch_size=100, epochs=5,\n",
        "              validation_data=[[x_test, y_test], [y_test, x_test]])\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 28, 28, 1)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1 (Conv2D)                  (None, 20, 20, 256)  20992       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "primarycap_conv2d (Conv2D)      (None, 6, 6, 256)    5308672     conv1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "primarycap_reshape (Reshape)    (None, 1152, 8)      0           primarycap_conv2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "primarycap_squash (Lambda)      (None, 1152, 8)      0           primarycap_reshape[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "digitcaps (CapsuleLayer)        (None, 10, 16)       1474560     primarycap_squash[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, 10)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "mask_1 (Mask)                   (None, 160)          0           digitcaps[0][0]                  \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "capsnet (Length)                (None, 10)           0           digitcaps[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "decoder (Sequential)            (None, 28, 28, 1)    1411344     mask_1[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 8,215,568\n",
            "Trainable params: 8,215,568\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "60000/60000 [==============================] - 145s 2ms/step - loss: 0.5079 - capsnet_loss: 0.4815 - decoder_loss: 0.0672 - capsnet_acc: 0.4705 - val_loss: 0.0682 - val_capsnet_loss: 0.0484 - val_decoder_loss: 0.0503 - val_capsnet_acc: 0.9671\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 143s 2ms/step - loss: 0.0563 - capsnet_loss: 0.0385 - decoder_loss: 0.0456 - capsnet_acc: 0.9735 - val_loss: 0.0462 - val_capsnet_loss: 0.0301 - val_decoder_loss: 0.0411 - val_capsnet_acc: 0.9805\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 143s 2ms/step - loss: 0.0421 - capsnet_loss: 0.0273 - decoder_loss: 0.0379 - capsnet_acc: 0.9817 - val_loss: 0.0383 - val_capsnet_loss: 0.0243 - val_decoder_loss: 0.0356 - val_capsnet_acc: 0.9847\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 144s 2ms/step - loss: 0.0358 - capsnet_loss: 0.0228 - decoder_loss: 0.0333 - capsnet_acc: 0.9858 - val_loss: 0.0344 - val_capsnet_loss: 0.0219 - val_decoder_loss: 0.0318 - val_capsnet_acc: 0.9838\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 145s 2ms/step - loss: 0.0317 - capsnet_loss: 0.0197 - decoder_loss: 0.0305 - capsnet_acc: 0.9876 - val_loss: 0.0306 - val_capsnet_loss: 0.0191 - val_decoder_loss: 0.0294 - val_capsnet_acc: 0.9878\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc9368a6eb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbWj2s6XpSFh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}